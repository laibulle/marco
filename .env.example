# LLM Provider Configuration
# For development: Use ollama (free, local, no API key needed)
# For production: Use openai or anthropic

# Choose provider: ollama (local dev), openai, or anthropic
LLM_PROVIDER=llamacpp
LLAMACPP_SERVER_URL=http://localhost:8080
LLAMACPP_MAX_TOKENS=2048
LLAMACPP_TEMPERATURE=0.7
LLAMACPP_N_CTX=4096

# Ollama Configuration (for local development)
OLLAMA_MODEL=qwen2.5:4b
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration (for production)
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-4o

# Anthropic Configuration (alternative for production)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# llamacpp Configuration (for local custom models)
# Option 1: Server mode (connect to llama.cpp server)
# LLAMACPP_SERVER_URL=http://localhost:8080
# Option 2: Local file mode (load GGUF file directly)
# LLAMACPP_MODEL_PATH=/path/to/your/model.gguf
# LLAMACPP_MAX_TOKENS=2048
# LLAMACPP_TEMPERATURE=0.7
# LLAMACPP_N_CTX=4096

# Optional: Database path
DATABASE_PATH=~/.marco/marco.db

# Optional: User preferences
DEFAULT_REGION=europe
DEFAULT_SEASON=auto
